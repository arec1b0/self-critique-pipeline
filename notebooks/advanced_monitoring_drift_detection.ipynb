{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Monitoring & Drift Detection\n",
        "\n",
        "This notebook focuses on advanced monitoring techniques to ensure the long-term reliability and quality of the Self-Critique pipeline. It provides tools for detecting data drift, concept drift, and model performance degradation.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Data Drift Detection**: Identify changes in the statistical properties of input data.\n",
        "- **Model Drift Detection**: Monitor for degradation in the pipeline's quality scores over time.\n",
        "- **Statistical Monitoring**: Apply statistical tests (e.g., Kolmogorov-Smirnov) to detect drift.\n",
        "- **Alerting**: Establish a framework for automated drift alerts.\n",
        "- **Remediation Strategies**: Understand how to respond to detected drift.\n",
        "\n",
        "## Business Context\n",
        "\n",
        "LLM-based systems can degrade silently over time as input data characteristics or user expectations change. Proactive drift detection is essential to maintain quality and trust. This notebook helps answer:\n",
        "\n",
        "- Is the pipeline performing as well today as it did last month?\n",
        "- Has the type of content we're processing changed?\n",
        "- How can we automatically detect and get alerted to quality degradation?\n",
        "- When should we consider updating our prompts or models?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "from notebooks._shared_utilities import (\n",
        "    load_monitoring_data,\n",
        "    compare_distributions\n",
        ")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "\n",
        "print(\"âœ“ Environment setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Simulating Monitoring Data\n",
        "\n",
        "We'll create a simulated dataset representing pipeline performance over time. This will include a \"baseline\" period of normal performance and a \"current\" period where we'll introduce drift.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_monitoring_data(num_records: int, drift: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"Generates a DataFrame of simulated monitoring logs.\"\"\"\n",
        "    base_date = pd.to_datetime('2024-01-01')\n",
        "    dates = [base_date + pd.Timedelta(hours=i) for i in range(num_records)]\n",
        "    \n",
        "    data = {\n",
        "        'timestamp': dates,\n",
        "        'paper_length': np.random.normal(3000, 500, num_records),\n",
        "        'overall_quality': np.random.normal(8.8, 0.4, num_records),\n",
        "        'latency_seconds': np.random.normal(6.0, 1.0, num_records)\n",
        "    }\n",
        "    \n",
        "    if drift:\n",
        "        # Introduce drift in the second half of the data\n",
        "        drift_point = num_records // 2\n",
        "        data['paper_length'][drift_point:] = np.random.normal(4500, 700, num_records - drift_point) # Data drift\n",
        "        data['overall_quality'][drift_point:] = np.random.normal(7.2, 0.8, num_records - drift_point) # Model drift\n",
        "        data['latency_seconds'][drift_point:] = np.random.normal(8.5, 1.5, num_records - drift_point) # Performance drift\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate baseline and current data\n",
        "baseline_data = simulate_monitoring_data(1000, drift=False)\n",
        "current_data = simulate_monitoring_data(1000, drift=True)\n",
        "\n",
        "# Split current data into pre-drift and post-drift for visualization\n",
        "current_baseline = current_data.iloc[:500]\n",
        "current_drifted = current_data.iloc[500:]\n",
        "\n",
        "print(\"Simulated Data Summary:\")\n",
        "print(\"\\nBaseline Data:\")\n",
        "print(baseline_data.describe())\n",
        "print(\"\\nCurrent (Drifted) Data:\")\n",
        "print(current_drifted.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Data Drift Detection\n",
        "\n",
        "Data drift occurs when the statistical properties of the input data change. We'll monitor `paper_length` as a proxy for input complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_drift(baseline, current, metric, ax, title):\n",
        "    \"\"\"Helper to plot and compare distributions.\"\"\"\n",
        "    sns.kdeplot(baseline[metric], ax=ax, label='Baseline', color='blue', fill=True)\n",
        "    sns.kdeplot(current[metric], ax=ax, label='Current', color='red', fill=True)\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    \n",
        "    # Perform KS test\n",
        "    stat, p_value = compare_distributions(baseline[metric], current[metric], test='ks')\n",
        "    drift_detected = p_value < 0.05\n",
        "    verdict = \"Drift Detected\" if drift_detected else \"No Drift\"\n",
        "    ax.text(0.95, 0.95, f'p-value: {p_value:.4f}\\n{verdict}', \n",
        "            transform=ax.transAxes, ha='right', va='top', \n",
        "            bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "    return drift_detected\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Drift Analysis: Baseline vs. Current', fontsize=16)\n",
        "\n",
        "# Data Drift (Paper Length)\n",
        "plot_drift(baseline_data, current_drifted, 'paper_length', axes[0], 'Data Drift: Paper Length')\n",
        "\n",
        "# Model Performance Drift (Quality Score)\n",
        "plot_drift(baseline_data, current_drifted, 'overall_quality', axes[1], 'Model Drift: Quality Score')\n",
        "\n",
        "# System Performance Drift (Latency)\n",
        "plot_drift(baseline_data, current_drifted, 'latency_seconds', axes[2], 'Performance Drift: Latency')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Time Series Drift Monitoring\n",
        "\n",
        "We can also monitor metrics over time using a sliding window to detect gradual drift.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def monitor_over_time(data: pd.DataFrame, metric: str, window_size: int = 100):\n",
        "    \"\"\"Calculates rolling statistics to monitor drift over time.\"\"\"\n",
        "    data[f'{metric}_rolling_mean'] = data[metric].rolling(window=window_size).mean()\n",
        "    data[f'{metric}_rolling_std'] = data[metric].rolling(window=window_size).std()\n",
        "    return data\n",
        "\n",
        "monitored_data = monitor_over_time(current_data, 'overall_quality')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 6))\n",
        "ax.plot(monitored_data['timestamp'], monitored_data['overall_quality'], 'k.', alpha=0.1, label='Raw Score')\n",
        "ax.plot(monitored_data['timestamp'], monitored_data['overall_quality_rolling_mean'], 'b-', label='Rolling Mean')\n",
        "ax.fill_between(monitored_data['timestamp'], \n",
        "                monitored_data['overall_quality_rolling_mean'] - 2 * monitored_data['overall_quality_rolling_std'], \n",
        "                monitored_data['overall_quality_rolling_mean'] + 2 * monitored_data['overall_quality_rolling_std'], \n",
        "                color='blue', alpha=0.2, label='Â±2 Std Dev')\n",
        "\n",
        "ax.set_title('Time Series Monitoring for Quality Score Drift')\n",
        "ax.set_xlabel('Timestamp')\n",
        "ax.set_ylabel('Overall Quality Score')\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Add a horizontal line for the baseline mean\n",
        "baseline_mean = baseline_data['overall_quality'].mean()\n",
        "ax.axhline(baseline_mean, color='red', linestyle='--', label='Baseline Mean')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Automated Drift Alerts\n",
        "\n",
        "This section outlines a simple alerting mechanism that could be integrated into a production monitoring system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DriftDetector:\n",
        "    def __init__(self, baseline_df: pd.DataFrame, p_value_threshold: float = 0.05):\n",
        "        self.baseline_df = baseline_df\n",
        "        self.p_value_threshold = p_value_threshold\n",
        "        print(f\"Drift detector initialized with baseline. p-value threshold: {p_value_threshold}\")\n",
        "\n",
        "    def check(self, current_df: pd.DataFrame) -> List[str]:\n",
        "        \"\"\"Checks for drift in key metrics and returns a list of alerts.\"\"\"\n",
        "        alerts = []\n",
        "        metrics_to_check = ['paper_length', 'overall_quality', 'latency_seconds']\n",
        "\n",
        "        for metric in metrics_to_check:\n",
        "            _, p_value = compare_distributions(self.baseline_df[metric], current_df[metric])\n",
        "            if p_value < self.p_value_threshold:\n",
        "                alert_msg = f\"ðŸš¨ DRIFT DETECTED in '{metric}' (p-value: {p_value:.4f})\"\n",
        "                print(alert_msg)\n",
        "                alerts.append(alert_msg)\n",
        "        \n",
        "        if not alerts:\n",
        "            print(\"âœ… No significant drift detected.\")\n",
        "            \n",
        "        return alerts\n",
        "\n",
        "# Initialize the detector with the baseline data\n",
        "detector = DriftDetector(baseline_data)\n",
        "\n",
        "print(\"\\n--- Checking for drift in the first half (no drift) ---\")\n",
        "alerts_no_drift = detector.check(current_baseline)\n",
        "\n",
        "print(\"\\n--- Checking for drift in the second half (drift) ---\")\n",
        "alerts_drift = detector.check(current_drifted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Remediation Strategies\n",
        "\n",
        "When drift is detected, several actions can be taken:\n",
        "\n",
        "| Drift Type | Potential Causes | Remediation Strategies |\n",
        "| :--- | :--- | :--- |\n",
        "| **Data Drift** | New data sources, changing topics, different document formats | - **Update Prompts**: Modify prompts to handle new data characteristics.<br>- **Retrain/Fine-tune**: If using a fine-tuned model, retrain on recent data.<br>- **Data Validation**: Add stricter validation at the input layer. |\n",
        "| **Model Drift** | Data drift, concept drift (meaning of quality changes), model staleness | - **Prompt Engineering**: A/B test new prompts to improve performance.<br>- **Model Upgrade**: Evaluate a newer, more capable model.<br>- **Human-in-the-Loop**: Collect human feedback on recent outputs to understand the failure mode. |\n",
        "| **Performance Drift**| API provider issues, larger inputs/outputs, inefficient code | - **Optimize Prompts**: Reduce token usage.<br>- **Caching**: Implement caching for common requests.<br>- **Infrastructure Scaling**: Increase resources if it's a bottleneck. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook provides a framework for monitoring and detecting drift in the Self-Critique pipeline. Key takeaways:\n",
        "\n",
        "1. **Multi-faceted Monitoring**: It's crucial to monitor data, model quality, and system performance metrics.\n",
        "2. **Statistical Rigor**: Statistical tests like the Kolmogorov-Smirnov test provide a robust way to quantify drift.\n",
        "3. **Automation is Key**: Automated monitoring and alerting are essential for catching issues before they impact users.\n",
        "4. **Have a Plan**: A clear set of remediation strategies is necessary to act on drift alerts effectively.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Integrate with a Monitoring Stack**: Export these metrics to a system like Prometheus and build dashboards in Grafana.\n",
        "2. **Set Up Automated Alerting**: Connect the `DriftDetector` to an alerting system like PagerDuty or Slack.\n",
        "3. **Establish a Re-evaluation Cadence**: Schedule regular, automated re-evaluation of the pipeline against the benchmark dataset from `model_evaluation_qa.ipynb`."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}