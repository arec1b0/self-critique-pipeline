{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DVC Pipeline Integration\n",
        "\n",
        "This notebook provides a guide to integrating the Self-Critique pipeline with DVC (Data Version Control) for versioning prompts, datasets, and experiment metrics. This ensures full reproducibility of your results.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Prompt Versioning**: Use DVC to track changes to prompt templates.\n",
        "- **Dataset Versioning**: Version the benchmark dataset to ensure consistent evaluation.\n",
        "- **Metrics Tracking**: Log experiment metrics with DVC for comparison.\n",
        "- **Reproducibility**: Create a fully reproducible pipeline from data to results.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: DVC Setup for Prompts and Data\n",
        "\n",
        "First, we'll place our prompts and benchmark dataset under DVC control.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "bash"
        ]
      },
      "outputs": [],
      "source": [
        "# Assumes DVC is initialized (`dvc init`)\n",
        "\n",
        "# Create directories for prompts and data\n",
        "mkdir -p ../../prompts ../../data/benchmark\n",
        "\n",
        "# Create a prompt template\n",
        "echo \"Summarize the following paper: {{text}}\" > ../../prompts/summary_v1.prompt\n",
        "\n",
        "# Create a dummy benchmark dataset\n",
        "echo '{\"title\": \"Paper 1\", \"text\": \"...\"}' > ../../data/benchmark/paper1.json\n",
        "echo '{\"title\": \"Paper 2\", \"text\": \"...\"}' > ../../data/benchmark/paper2.json\n",
        "\n",
        "# Add these directories to DVC\n",
        "# This creates .dvc files that point to the data\n",
        "dvc add ../../prompts ../../data/benchmark\n",
        "\n",
        "# You would then commit these .dvc files to Git\n",
        "# git add prompts.dvc data/benchmark.dvc\n",
        "# git commit -m \"Track prompts and benchmark data with DVC\"\n",
        "\n",
        "print(\"✓ Prompts and data directories are now tracked by DVC.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: DVC Pipeline for Evaluation\n",
        "\n",
        "DVC can define and run a multi-stage pipeline. We'll create a simple evaluation pipeline that takes our prompts and data as inputs and produces metrics as an output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "bash"
        ]
      },
      "outputs": [],
      "source": [
        "%%writefile ../../evaluate.py\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import yaml\n",
        "import sys\n",
        "\n",
        "def evaluate(prompt_path, data_dir, metrics_file):\n",
        "    \"\"\"Simulates running an evaluation.\"\"\"\n",
        "    print(f\"Using prompt: {prompt_path}\")\n",
        "    print(f\"Using data from: {data_dir}\")\n",
        "    \n",
        "    # Read parameters from params.yaml\n",
        "    params = yaml.safe_load(open(\"params.yaml\"))\n",
        "    \n",
        "    # Simulate evaluation\n",
        "    avg_quality = 8.5 + params['temperature'] * 0.5 + random.uniform(-0.2, 0.2)\n",
        "    \n",
        "    metrics = {\n",
        "        'avg_quality': avg_quality,\n",
        "        'model': params['model'],\n",
        "        'temperature': params['temperature']\n",
        "    }\n",
        "    \n",
        "    os.makedirs(os.path.dirname(metrics_file), exist_ok=True)\n",
        "    with open(metrics_file, 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "        \n",
        "    print(f\"Metrics saved to {metrics_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt_path = sys.argv[1]\n",
        "    data_dir = sys.argv[2]\n",
        "    metrics_file = sys.argv[3]\n",
        "    evaluate(prompt_path, data_dir, metrics_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Defining Parameters and Pipeline Stages\n",
        "\n",
        "We'll create a `params.yaml` for parameters and a `dvc.yaml` to define the pipeline stages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "bash"
        ]
      },
      "outputs": [],
      "source": [
        "# Create a params.yaml to hold parameters\n",
        "echo 'model: claude-sonnet-4-20250514' > ../../params.yaml\n",
        "echo 'temperature: 0.3' >> ../../params.yaml\n",
        "\n",
        "# Create the dvc.yaml to define the pipeline\n",
        "cat <<EOL > ../../dvc.yaml\n",
        "stages:\n",
        "  evaluate:\n",
        "    cmd: python evaluate.py prompts/summary_v1.prompt data/benchmark metrics.json\n",
        "    deps:\n",
        "    - evaluate.py\n",
        "    - prompts/summary_v1.prompt\n",
        "    - data/benchmark\n",
        "    params:\n",
        "    - model\n",
        "    - temperature\n",
        "    metrics:\n",
        "    - metrics.json:\n",
        "        cache: false\n",
        "EOL\n",
        "\n",
        "print(\"✓ dvc.yaml and params.yaml created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Running Experiments and Comparing Metrics\n",
        "\n",
        "DVC makes it easy to run experiments with different parameters and compare the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "bash"
        ]
      },
      "outputs": [],
      "source": [
        "# Run the pipeline for the first time\n",
        "dvc exp run --name baseline\n",
        "\n",
        "# Change a parameter and run another experiment\n",
        "dvc exp run --set-param temperature=0.7 --name high-temp\n",
        "\n",
        "# Show the results\n",
        "dvc exp show\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation of `dvc exp show`\n",
        "\n",
        "The output will show a table comparing the two experiments (`baseline` and `high-temp`). You'll see the different `temperature` parameter and the resulting `avg_quality` metric, allowing for easy comparison.\n",
        "\n",
        "## Section 4: Rollback and Reproducibility\n",
        "\n",
        "If an experiment shows a drop in quality, you can easily revert to a previous version of your code, data, and prompts.\n",
        "\n",
        "```bash\n",
        "# To go back to the baseline experiment\n",
        "dvc exp apply baseline\n",
        "\n",
        "# This will revert your code, params.yaml, and any data files\n",
        "# to the state they were in for that experiment.\n",
        "```\n",
        "\n",
        "This ensures that anyone can reproduce your results by checking out the same Git commit and running `dvc repro`.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
