{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Tuning & Latency Optimization\n",
        "\n",
        "This notebook provides a framework for analyzing and optimizing the performance of the Self-Critique Chain Pipeline, with a focus on reducing latency and increasing throughput.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Latency Analysis**: Profile pipeline execution to identify bottlenecks.\n",
        "- **Parameter Tuning**: Understand the impact of temperature and `max_tokens` on performance.\n",
        "- **Caching Strategies**: Implement and evaluate caching to reduce redundant API calls.\n",
        "- **Batch Processing**: Explore strategies for processing multiple documents efficiently.\n",
        "- **Throughput Measurement**: Measure the maximum sustainable throughput of the system.\n",
        "\n",
        "## Business Context\n",
        "\n",
        "For user-facing applications, low latency is critical for a good user experience. For large-scale batch processing, high throughput is essential for efficiency. This notebook helps answer:\n",
        "\n",
        "- What is the end-to-end latency of the pipeline (P50, P95, P99)?\n",
        "- Which stage of the pipeline is the slowest?\n",
        "- How can we reduce latency without significantly impacting quality?\n",
        "- How can we maximize the number of papers processed per hour?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import lru_cache\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "from src.pipeline import SelfCritiquePipeline\n",
        "from notebooks._shared_utilities import (\n",
        "    create_benchmark_dataset,\n",
        "    calculate_percentiles,\n",
        "    format_duration\n",
        ")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "\n",
        "print(\"âœ“ Environment setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Latency Profiling\n",
        "\n",
        "First, we'll measure the latency of a single pipeline execution and break it down by stage to identify bottlenecks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profile_pipeline_execution(pipeline: SelfCritiquePipeline, text: str) -> Dict[str, float]:\n",
        "    \"\"\"Profiles a single run of the pipeline and returns latency metrics.\"\"\"\n",
        "    # In a real scenario, this would come from pipeline.run_pipeline()\n",
        "    # For now, we simulate it.\n",
        "    stage_latencies = {\n",
        "        'stage1_summary': np.random.uniform(1.5, 2.5),\n",
        "        'stage2_critique': np.random.uniform(2.0, 3.0),\n",
        "        'stage3_revision': np.random.uniform(1.8, 2.8),\n",
        "    }\n",
        "    total_latency = sum(stage_latencies.values())\n",
        "    stage_latencies['total'] = total_latency\n",
        "    return stage_latencies\n",
        "\n",
        "# Load a sample paper\n",
        "sample_paper = create_benchmark_dataset()[0]\n",
        "pipeline = SelfCritiquePipeline(api_key=\"DUMMY_KEY\")\n",
        "\n",
        "# Profile the execution\n",
        "latency_profile = profile_pipeline_execution(pipeline, sample_paper['text'])\n",
        "\n",
        "# Visualize the breakdown\n",
        "latency_df = pd.DataFrame.from_dict(latency_profile, orient='index', columns=['latency_seconds'])\n",
        "latency_df.sort_values('latency_seconds', ascending=False, inplace=True)\n",
        "\n",
        "ax = latency_df.plot(kind='barh', legend=False, color='skyblue')\n",
        "ax.set_title('Latency Breakdown by Pipeline Stage')\n",
        "ax.set_xlabel('Latency (seconds)')\n",
        "plt.show()\n",
        "\n",
        "print(\"Latency Profile:\")\n",
        "for stage, latency in latency_profile.items():\n",
        "    print(f\"- {stage:<20}: {format_duration(latency)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Temperature Parameter Impact\n",
        "\n",
        "We'll analyze how the `temperature` parameter affects latency and (qualitatively) output diversity. Lower temperatures are often faster and more deterministic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "temperatures = [0.0, 0.3, 0.5, 0.7, 1.0]\n",
        "latency_results = []\n",
        "\n",
        "for temp in temperatures:\n",
        "    # In a real test, you'd set pipeline.temperature = temp\n",
        "    # Here, we simulate the effect: higher temp -> slightly higher latency\n",
        "    simulated_latency = 5.0 + (temp * 1.5) + np.random.uniform(-0.2, 0.2)\n",
        "    latency_results.append({'temperature': temp, 'latency': simulated_latency})\n",
        "\n",
        "temp_df = pd.DataFrame(latency_results)\n",
        "\n",
        "ax = sns.lineplot(data=temp_df, x='temperature', y='latency', marker='o')\n",
        "ax.set_title('Impact of Temperature on Latency')\n",
        "ax.set_xlabel('Temperature')\n",
        "ax.set_ylabel('Average Latency (seconds)')\n",
        "plt.show()\n",
        "\n",
        "print(temp_df)"
      ]
    },
        {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Caching Implementation\n",
        "\n",
        "Implementing a cache can dramatically reduce latency for repeated requests with the same input. We'll use a simple in-memory LRU (Least Recently Used) cache.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=128)\n",
        "def cached_pipeline_run(text: str):\n",
        "    \"\"\"Simulates a cached pipeline run.\"\"\"\n",
        "    time.sleep(np.random.uniform(0.5, 1.5)) # Simulate work\n",
        "    return f\"Summary for: {text[:50]}...\"\n",
        "\n",
        "sample_texts = [p['text'] for p in create_benchmark_dataset()] * 3 # Create duplicates\n",
        "\n",
        "# Time the uncached version\n",
        "start_time_uncached = time.time()\n",
        "for text in sample_texts:\n",
        "    _ = cached_pipeline_run.__wrapped__(text)\n",
        "end_time_uncached = time.time()\n",
        "uncached_duration = end_time_uncached - start_time_uncached\n",
        "\n",
        "# Time the cached version\n",
        "cached_pipeline_run.cache_clear()\n",
        "start_time_cached = time.time()\n",
        "for text in sample_texts:\n",
        "    _ = cached_pipeline_run(text)\n",
        "end_time_cached = time.time()\n",
        "cached_duration = end_time_cached - start_time_cached\n",
        "\n",
        "print(f\"Uncached execution time: {format_duration(uncached_duration)}\")\n",
        "print(f\"Cached execution time:   {format_duration(cached_duration)}\")\n",
        "print(f\"Performance Improvement: {uncached_duration / cached_duration:.2f}x\")\n",
        "print(f\"Cache Info: {cached_pipeline_run.cache_info()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Batch Processing & Throughput Analysis\n",
        "\n",
        "We'll measure throughput by running multiple requests in parallel using a thread pool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_batch(texts: List[str], max_workers: int) -> float:\n",
        "    \"\"\"Runs a batch of requests in parallel and returns total time.\"\"\"\n",
        "    start_time = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        tasks = [loop.run_in_executor(executor, cached_pipeline_run, text) for text in texts]\n",
        "        await asyncio.gather(*tasks)\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
        "throughput_results = []\n",
        "dataset = [p['text'] for p in create_benchmark_dataset()]\n",
        "cached_pipeline_run.cache_clear()\n",
        "\n",
        "async def main():\n",
        "    for workers in batch_sizes:\n",
        "        duration = await run_batch(dataset, max_workers=workers)\n",
        "        throughput = len(dataset) / duration\n",
        "        throughput_results.append({'workers': workers, 'duration': duration, 'throughput': throughput})\n",
        "        print(f\"Workers: {workers:<2} | Duration: {format_duration(duration):<10} | Throughput: {throughput:.2f} papers/sec\")\n",
        "\n",
        "# Run the async main function\n",
        "await main()\n",
        "\n",
        "throughput_df = pd.DataFrame(throughput_results)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "sns.lineplot(data=throughput_df, x='workers', y='duration', marker='o', ax=axes[0])\n",
        "axes[0].set_title('Batch Processing Duration')\n",
        "axes[0].set_xlabel('Number of Parallel Workers')\n",
        "axes[0].set_ylabel('Total Duration (seconds)')\n",
        "\n",
        "sns.lineplot(data=throughput_df, x='workers', y='throughput', marker='o', ax=axes[1])\n",
        "axes[1].set_title('Throughput vs. Parallel Workers')\n",
        "axes[1].set_xlabel('Number of Parallel Workers')\n",
        "axes[1].set_ylabel('Throughput (papers/second)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook provides the tools to analyze and optimize pipeline performance. Key takeaways:\n",
        "\n",
        "1. **Identify Bottlenecks**: Stage-level latency profiling is crucial for focusing optimization efforts.\n",
        "2. **Tune Parameters**: `temperature` and `max_tokens` offer a trade-off between speed, cost, and quality.\n",
        "3. **Implement Caching**: Caching is the most effective strategy for high-hit-rate scenarios.\n",
        "4. **Parallelize for Throughput**: Using parallel execution with a thread pool can significantly boost throughput, though returns diminish as external service limits are reached.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Load Testing**: Use a tool like Locust to simulate real-world load and identify scaling limits.\n",
        "2. **Asyncio Native Client**: For maximum performance, use an `asyncio`-native HTTP client (like `aiohttp`) instead of a thread pool.\n",
        "3. **Cost vs. Performance**: Integrate cost analysis from `cost_economics_analysis.ipynb` to find the optimal balance between speed and cost.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}