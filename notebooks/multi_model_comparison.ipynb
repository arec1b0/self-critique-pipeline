{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Model Comparison & Optimization\n",
        "\n",
        "This notebook provides a framework for systematically comparing different language models (e.g., Haiku, Sonnet, Opus) to find the optimal balance of cost, quality, and latency for the Self-Critique pipeline.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Model Benchmarking**: Compare the performance of different models on a standardized dataset.\n",
        "- **Parameter Grid Search**: Systematically test different temperature and `max_tokens` settings.\n",
        "- **Cost-Quality Analysis**: Visualize the Pareto frontier to make informed trade-offs.\n",
        "- **Hybrid Strategies**: Explore using different models for different pipeline stages.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import itertools\n",
        "\n",
        "from notebooks._shared_utilities import (\n",
        "    create_benchmark_dataset,\n",
        "    calculate_cost_metrics,\n",
        "    calculate_quality_metrics\n",
        ")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "\n",
        "print(\"âœ“ Environment setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Model Comparison Experiment\n",
        "\n",
        "We'll run the same benchmark dataset through three different models: Haiku (fastest, cheapest), Sonnet (balanced), and Opus (highest quality).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(models_to_test: list, dataset: list) -> pd.DataFrame:\n",
        "    \"\"\"Runs a multi-model experiment and returns a DataFrame of results.\"\"\"\n",
        "    results = []\n",
        "    for model in tqdm(models_to_test, desc=\"Testing Models\"):\n",
        "        for paper in tqdm(dataset, desc=f\"Processing with {model}\", leave=False):\n",
        "            # Simulate a pipeline run\n",
        "            # Replace with a real pipeline call\n",
        "            simulated_latency = 10 if 'haiku' in model else 6 if 'sonnet' in model else 15\n",
        "            simulated_quality = 7.5 if 'haiku' in model else 8.5 if 'sonnet' in model else 9.5\n",
        "            \n",
        "            run_result = {\n",
        "                'model': model,\n",
        "                'paper': paper['title'],\n",
        "                'total_metrics': {\n",
        "                    'total_input_tokens': np.random.randint(2800, 3200),\n",
        "                    'total_output_tokens': np.random.randint(800, 1200),\n",
        "                    'total_latency_seconds': simulated_latency + np.random.uniform(-1, 1)\n",
        "                },\n",
        "                'critique': f\"Overall: {simulated_quality + np.random.uniform(-0.3, 0.3)}/10\"\n",
        "            }\n",
        "            \n",
        "            cost_metrics = calculate_cost_metrics(run_result, model=model)\n",
        "            quality_metrics = calculate_quality_metrics(run_result)\n",
        "            \n",
        "            results.append({\n",
        "                'model': model,\n",
        "                'latency': run_result['total_metrics']['total_latency_seconds'],\n",
        "                'cost': cost_metrics['total_cost_usd'],\n",
        "                'quality': quality_metrics['overall']\n",
        "            })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "models = [\"claude-haiku-4-20250514\", \"claude-sonnet-4-20250514\", \"claude-opus-4-20250514\"]\n",
        "benchmark_dataset = create_benchmark_dataset()\n",
        "\n",
        "experiment_results = run_experiment(models, benchmark_dataset)\n",
        "\n",
        "# Aggregate results\n",
        "summary = experiment_results.groupby('model').mean().reset_index()\n",
        "print(\"Experiment Summary:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Cost-Quality Pareto Frontier\n",
        "\n",
        "A Pareto frontier helps visualize the trade-offs between cost and quality. The optimal models lie on the edge of the curve.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "sns.scatterplot(data=summary, x='cost', y='quality', hue='model', s=200, ax=ax, palette='viridis')\n",
        "\n",
        "ax.set_title('Cost vs. Quality Trade-off', fontsize=16)\n",
        "ax.set_xlabel('Average Cost per Execution (USD)')\n",
        "ax.set_ylabel('Average Quality Score')\n",
        "ax.grid(True)\n",
        "\n",
        "# Annotate points\n",
        "for i, row in summary.iterrows():\n",
        "    ax.text(row['cost'] * 1.01, row['quality'], row['model'].split('-')[1], fontsize=12)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretation:\")\n",
        "print(\"- Haiku: Lowest cost, lowest quality.\")\n",
        "print(\"- Opus: Highest quality, highest cost.\")\n",
        "print(\"- Sonnet: A balanced choice, offering a good quality improvement over Haiku for a moderate cost increase.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Stage-Specific Model Selection (Hybrid Strategy)\n",
        "\n",
        "A powerful optimization is to use different models for different stages. For example, use a cheaper model for the initial summary and a more powerful one for the critique and revision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_hybrid_cost(stage1_model, stage2_model, stage3_model):\n",
        "    \"\"\"Simulates the cost of a hybrid model strategy.\"\"\"\n",
        "    # Dummy token counts for each stage\n",
        "    stage_tokens = {\n",
        "        'stage1': {'input': 3000, 'output': 800},\n",
        "        'stage2': {'input': 3800, 'output': 400},\n",
        "        'stage3': {'input': 4200, 'output': 800}\n",
        "    }\n",
        "    \n",
        "    models_used = [stage1_model, stage2_model, stage3_model]\n",
        "    total_cost = 0\n",
        "    \n",
        "    for i, model in enumerate(models_used):\n",
        "        stage_name = f'stage{i+1}'\n",
        "        stage_result = {'total_metrics': {'total_input_tokens': stage_tokens[stage_name]['input'], 'total_output_tokens': stage_tokens[stage_name]['output']}}\n",
        "        cost_metrics = calculate_cost_metrics(stage_result, model=model)\n",
        "        total_cost += cost_metrics['total_cost_usd']\n",
        "    \n",
        "    return total_cost\n",
        "\n",
        "# Compare strategies\n",
        "sonnet_only_cost = calculate_hybrid_cost(models[1], models[1], models[1])\n",
        "hybrid_cost = calculate_hybrid_cost(models[0], models[2], models[2]) # Haiku for summary, Opus for critique/revision\n",
        "\n",
        "print(f\"Sonnet-Only Strategy Cost: ${sonnet_only_cost:.4f}\")\n",
        "print(f\"Hybrid (Haiku/Opus) Strategy Cost: ${hybrid_cost:.4f}\")\n",
        "print(f\"Savings: {(sonnet_only_cost - hybrid_cost) / sonnet_only_cost:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Temperature Grid Search\n",
        "\n",
        "Fine-tuning the `temperature` parameter can impact both quality and cost (by affecting output token count).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is a conceptual example. A real grid search would take a long time.\n",
        "temperatures = [0.1, 0.3, 0.5, 0.7]\n",
        "max_tokens = [1024, 2048]\n",
        "\n",
        "grid_search_params = list(itertools.product(temperatures, max_tokens))\n",
        "grid_search_results = []\n",
        "\n",
        "for temp, tokens in grid_search_params:\n",
        "    # Simulate a run with these params\n",
        "    simulated_quality = 8.5 - abs(0.5 - temp) # Assume optimal temp is 0.5\n",
        "    simulated_cost = 0.015 * (tokens / 2048)\n",
        "    \n",
        "    grid_search_results.append({\n",
        "        'temperature': temp,\n",
        "        'max_tokens': tokens,\n",
        "        'quality': simulated_quality,\n",
        "        'cost': simulated_cost\n",
        "    })\n",
        "\n",
        "grid_df = pd.DataFrame(grid_search_results)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(grid_df['temperature'], grid_df['max_tokens'], grid_df['quality'], c=grid_df['cost'], cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('Temperature')\n",
        "ax.set_ylabel('Max Tokens')\n",
        "ax.set_zlabel('Quality Score')\n",
        "ax.set_title('Grid Search for Optimal Parameters')\n",
        "plt.show()\n",
        "\n",
        "print(\"Optimal Parameters (Highest Quality):\")\n",
        "print(grid_df.loc[grid_df['quality'].idxmax()])"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
