{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation & Quality Assurance\n",
        "\n",
        "This notebook establishes a rigorous framework for evaluating the quality of the Self-Critique Chain Pipeline. It provides tools and methodologies for ensuring reproducibility, preventing regressions, and making data-driven decisions about prompt engineering and model selection.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Benchmark Dataset**: Create a standardized dataset for consistent evaluation.\n",
        "- **Quality Metrics**: Define and implement a multi-dimensional quality framework.\n",
        "- **A/B Testing**: Compare different prompt templates or models systematically.\n",
        "- **Regression Testing**: Build an automated suite to prevent quality degradation.\n",
        "- **Quality Gates**: Establish clear thresholds for production readiness.\n",
        "\n",
        "## Business Context\n",
        "\n",
        "Maintaining high-quality output is critical for user trust and the reliability of downstream systems. This notebook addresses key questions:\n",
        "\n",
        "- How do we know if a new prompt is better than the old one?\n",
        "- How can we prevent a change from silently degrading quality?\n",
        "- What is the quality score of the current production model?\n",
        "- How do we select the best model based on empirical evidence?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Any\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "from src.pipeline import SelfCritiquePipeline\n",
        "from notebooks._shared_utilities import (\n",
        "    create_benchmark_dataset,\n",
        "    calculate_quality_metrics,\n",
        "    plot_quality_comparison,\n",
        "    compare_distributions,\n",
        "    setup_mlflow_context\n",
        ")\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 7)\n",
        "\n",
        "# Setup MLflow for experiment tracking\n",
        "setup_mlflow_context(experiment_name=\"model-evaluation-qa\")\n",
        "\n",
        "print(\"✓ Environment setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Benchmark Dataset Creation\n",
        "\n",
        "A standardized dataset is essential for reproducible evaluations. We use a diverse set of research paper abstracts to test the pipeline's performance across different domains.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark_dataset = create_benchmark_dataset()\n",
        "benchmark_df = pd.DataFrame(benchmark_dataset)\n",
        "\n",
        "print(f\"Benchmark dataset created with {len(benchmark_df)} papers.\")\n",
        "print(\"\\nDataset Summary:\")\n",
        "print(benchmark_df[['title', 'category']])\n",
        "\n",
        "print(\"\\nSample Paper Text:\")\n",
        "print(benchmark_df.iloc[0]['text'][:300] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Quality Metrics Framework\n",
        "\n",
        "We define a multi-dimensional framework to measure quality. These scores are extracted from the `critique` stage of the pipeline.\n",
        "\n",
        "- **Accuracy**: Factual correctness of the summary.\n",
        "- **Completeness**: Coverage of key points from the source.\n",
        "- **Clarity**: Readability and coherence of the text.\n",
        "- **Coherence**: Logical flow and consistency.\n",
        "- **Overall**: A holistic quality score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pipeline(pipeline: SelfCritiquePipeline, dataset: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Runs the pipeline over a dataset and collects metrics.\"\"\"\n",
        "    results = []\n",
        "    for paper in tqdm(dataset, desc=\"Evaluating Benchmark Dataset\"):\n",
        "        try:\n",
        "            # Simulate pipeline execution\n",
        "            # Replace with actual execution: pipeline.run_pipeline(paper['text'])\n",
        "            time.sleep(0.1) # Simulate network latency\n",
        "            simulated_result = {\n",
        "                \"summary\": \"This is a simulated summary.\",\n",
        "                \"critique\": f\"\"\"**Accuracy:** {np.random.randint(7, 10)}/10\\n**Completeness:** {np.random.randint(6, 10)}/10\\n**Clarity:** {np.random.randint(8, 10)}/10\\n**Coherence:** {np.random.randint(7, 10)}/10\\n**Overall:** {np.random.randint(7, 10)}/10\"\"\",\n",
        "                \"model\": pipeline.model,\n",
        "                \"paper_title\": paper['title']\n",
        "            }\n",
        "            \n",
        "            quality_scores = calculate_quality_metrics(simulated_result)\n",
        "            simulated_result.update(quality_scores)\n",
        "            results.append(simulated_result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing '{paper['title']}': {e}\")\n",
        "    return results\n",
        "\n",
        "# Initialize two pipeline versions for comparison\n",
        "# In a real scenario, these would have different prompt templates or model versions\n",
        "pipeline_v1 = SelfCritiquePipeline(api_key=\"DUMMY_KEY\", model=\"claude-sonnet-v1\")\n",
        "pipeline_v2 = SelfCritiquePipeline(api_key=\"DUMMY_KEY\", model=\"claude-sonnet-v2-improved-prompt\")\n",
        "\n",
        "# Evaluate both pipelines\n",
        "print(\"Evaluating Pipeline v1 (Baseline)\")\n",
        "v1_results = evaluate_pipeline(pipeline_v1, benchmark_dataset)\n",
        "v1_df = pd.DataFrame(v1_results)\n",
        "\n",
        "print(\"\\nEvaluating Pipeline v2 (Candidate)\")\n",
        "v2_results = evaluate_pipeline(pipeline_v2, benchmark_dataset)\n",
        "v2_df = pd.DataFrame(v2_results)\n",
        "\n",
        "print(\"\\nEvaluation complete.\")\n",
        "print(\"\\nBaseline (v1) Results Summary:\")\n",
        "print(v1_df[['overall', 'accuracy', 'completeness']].describe())\n",
        "\n",
        "print(\"\\nCandidate (v2) Results Summary:\")\n",
        "print(v2_df[['overall', 'accuracy', 'completeness']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: A/B Testing & Comparative Analysis\n",
        "\n",
        "We compare the performance of the two pipeline versions to determine if the new version offers a statistically significant improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_comparative_distributions(df1, df2, metric, ax, title):\n",
        "    \"\"\"Helper to plot comparative histograms.\"\"\"\n",
        "    sns.histplot(df1[metric], ax=ax, color='skyblue', label='v1 (Baseline)', kde=True, stat=\"density\")\n",
        "    sns.histplot(df2[metric], ax=ax, color='salmon', label='v2 (Candidate)', kde=True, stat=\"density\")\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "plot_comparative_distributions(v1_df, v2_df, 'overall', axes[0], 'Overall Score Distribution')\n",
        "plot_comparative_distributions(v1_df, v2_df, 'accuracy', axes[1], 'Accuracy Score Distribution')\n",
        "plot_comparative_distributions(v1_df, v2_df, 'completeness', axes[2], 'Completeness Score Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical Significance Testing\n",
        "print(\"Statistical Significance Testing (p-value < 0.05 indicates significant difference)\")\n",
        "print(\"=\"*80)\n",
        "metrics_to_test = ['overall', 'accuracy', 'completeness', 'clarity', 'coherence']\n",
        "\n",
        "for metric in metrics_to_test:\n",
        "    stat, p_value = compare_distributions(v1_df[metric], v2_df[metric], test='ks')\n",
        "    mean_v1 = v1_df[metric].mean()\n",
        "    mean_v2 = v2_df[metric].mean()\n",
        "    \n",
        "    verdict = \"✅ Significant\" if p_value < 0.05 else \"❌ Not Significant\"\n",
        "    improvement = f\"({mean_v2 - mean_v1:+.2f})\"\n",
        "    \n",
        "    print(f\"{metric.capitalize():<15} | p-value: {p_value:.4f} | {verdict:<20} | Improvement: {improvement}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Regression Testing Suite\n",
        "\n",
        "This suite runs automatically to ensure that code changes do not degrade quality. It compares the new version against a baseline (e.g., the main branch version).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RegressionTestSuite:\n",
        "    def __init__(self, baseline_results: pd.DataFrame, quality_threshold: float = 8.0):\n",
        "        self.baseline_results = baseline_results\n",
        "        self.quality_threshold = quality_threshold\n",
        "        print(f\"Regression suite initialized with baseline. Quality threshold set to > {quality_threshold}\")\n",
        "\n",
        "    def run(self, candidate_results: pd.DataFrame) -> bool:\n",
        "        \"\"\"Runs the regression test and returns a pass/fail status.\"\"\"\n",
        "        print(\"\\n--- Running Regression Test ---\")\n",
        "        passed = True\n",
        "        \n",
        "        # 1. Check for drop in average quality\n",
        "        baseline_mean = self.baseline_results['overall'].mean()\n",
        "        candidate_mean = candidate_results['overall'].mean()\n",
        "        \n",
        "        if candidate_mean < baseline_mean:\n",
        "            print(f\"❌ FAIL: Average quality dropped from {baseline_mean:.2f} to {candidate_mean:.2f}\")\n",
        "            passed = False\n",
        "        else:\n",
        "            print(f\"✅ PASS: Average quality improved or maintained ({baseline_mean:.2f} -> {candidate_mean:.2f})\")\n",
        "        \n",
        "        # 2. Check for minimum quality score\n",
        "        if candidate_mean < self.quality_threshold:\n",
        "            print(f\"❌ FAIL: Average quality {candidate_mean:.2f} is below threshold {self.quality_threshold}\")\n",
        "            passed = False\n",
        "        else:\n",
        "            print(f\"✅ PASS: Average quality {candidate_mean:.2f} meets threshold {self.quality_threshold}\")\n",
        "        \n",
        "        # 3. Check for statistically significant degradation\n",
        "        _, p_value = compare_distributions(self.baseline_results['overall'], candidate_results['overall'])\n",
        "        if p_value < 0.05 and candidate_mean < baseline_mean:\n",
        "            print(f\"❌ FAIL: Statistically significant quality degradation detected (p={p_value:.4f})\")\n",
        "            passed = False\n",
        "        else:\n",
        "            print(\"✅ PASS: No significant quality degradation detected\")\n",
        "        \n",
        "        print(\"--- Test Complete ---\")\n",
        "        return passed\n",
        "\n",
        "# Initialize and run the suite\n",
        "regression_suite = RegressionTestSuite(baseline_results=v1_df, quality_threshold=8.5)\n",
        "test_passed = regression_suite.run(candidate_results=v2_df)\n",
        "\n",
        "print(f\"\\nOverall Regression Test Result: {'PASSED' if test_passed else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Quality Gates and Thresholds\n",
        "\n",
        "Define clear, automated quality gates for CI/CD pipelines to prevent deploying low-quality models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quality_gate(results_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Automated quality gate for production deployment.\"\"\"\n",
        "    \n",
        "    thresholds = {\n",
        "        \"min_avg_overall_score\": 8.5,\n",
        "        \"min_avg_accuracy_score\": 8.0,\n",
        "        \"max_variance_overall\": 0.5,\n",
        "        \"max_failure_rate\": 0.01 # 1% failure rate\n",
        "    }\n",
        "    \n",
        "    checks = {}\n",
        "    passed = True\n",
        "    \n",
        "    # Check 1: Average Overall Score\n",
        "    avg_overall = results_df['overall'].mean()\n",
        "    checks['avg_overall_score'] = {\n",
        "        \"value\": avg_overall,\n",
        "        \"threshold\": thresholds['min_avg_overall_score'],\n",
        "        \"passed\": avg_overall >= thresholds['min_avg_overall_score']\n",
        "    }\n",
        "    if not checks['avg_overall_score']['passed']: passed = False\n",
        "\n",
        "    # Check 2: Average Accuracy Score\n",
        "    avg_accuracy = results_df['accuracy'].mean()\n",
        "    checks['avg_accuracy_score'] = {\n",
        "        \"value\": avg_accuracy,\n",
        "        \"threshold\": thresholds['min_avg_accuracy_score'],\n",
        "        \"passed\": avg_accuracy >= thresholds['min_avg_accuracy_score']\n",
        "    }\n",
        "    if not checks['avg_accuracy_score']['passed']: passed = False\n",
        "        \n",
        "    # Check 3: Score Variance\n",
        "    var_overall = results_df['overall'].var()\n",
        "    checks['score_variance'] = {\n",
        "        \"value\": var_overall,\n",
        "        \"threshold\": thresholds['max_variance_overall'],\n",
        "        \"passed\": var_overall <= thresholds['max_variance_overall']\n",
        "    }\n",
        "    if not checks['score_variance']['passed']: passed = False\n",
        "    \n",
        "    return {\"passed\": passed, \"checks\": checks}\n",
        "\n",
        "# Run quality gate on the candidate model\n",
        "gate_results = quality_gate(v2_df)\n",
        "\n",
        "print(\"Quality Gate Results for Candidate Model\")\n",
        "print(\"=\"*50)\n",
        "for check, result in gate_results['checks'].items():\n",
        "    status = \"✅ PASS\" if result['passed'] else \"❌ FAIL\"\n",
        "    print(f\"{check:<25}: {result['value']:.2f} (Threshold: {result['threshold']:.2f}) -> {status}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nDeployment Decision: {'APPROVE' if gate_results['passed'] else 'REJECT'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook provides a comprehensive framework for ensuring the quality and reliability of the Self-Critique pipeline. Key takeaways:\n",
        "\n",
        "1. **Standardized Evaluation**: The benchmark dataset enables consistent, reproducible testing.\n",
        "2. **Data-Driven Decisions**: A/B testing with statistical analysis provides clear evidence for model/prompt selection.\n",
        "3. **Safety Net**: The regression suite protects against unintentional quality degradation.\n",
        "4. **Automated Governance**: Quality gates provide a final, automated check before deployment.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Integrate into CI/CD**: Run the regression test and quality gate checks in your CI pipeline (e.g., GitHub Actions).\n",
        "2. **Expand Benchmark Dataset**: Add more diverse and challenging papers to the benchmark dataset.\n",
        "3. **Human-in-the-Loop**: Periodically sample outputs for human evaluation to calibrate and validate automated scores.\n",
        "4. **Track Over Time**: Log quality metrics in MLflow to monitor for long-term quality drift (see `advanced_monitoring_drift_detection.ipynb`)."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}